<h1 id="deep-sort">Deep SORT</h1>
<h2 id="introduction">Introduction</h2>
<p>This repository contains code for <em>Simple Online and Realtime Tracking with a Deep Association Metric</em> (Deep SORT). We extend the original <a href="https://github.com/abewley/sort">SORT</a> algorithm to integrate appearance information based on a deep appearance descriptor. See the <a href="https://arxiv.org/abs/1703.07402">arXiv preprint</a> for more information.</p>
<h2 id="dependencies">Dependencies</h2>
<p>The code is compatible with Python 2.7 and 3. The following dependencies are needed to run the tracker:</p>
<ul>
<li>NumPy</li>
<li>sklean</li>
<li>OpenCV</li>
</ul>
<p>Additionally, feature generation requires TensorFlow (&gt;= 1.0).</p>
<h2 id="installation">Installation</h2>
<p>First, clone the repository:</p>
<pre><code>git clone https://github.com/nwojke/deep_sort.git</code></pre>
<p>Then, download pre-generated detections and the CNN checkpoint file from <a href="https://owncloud.uni-koblenz.de/owncloud/s/f9JB0Jr7f3zzqs8">here</a>.</p>
<p><em>NOTE:</em> The candidate object locations of our pre-generated detections are taken from the following paper:</p>
<pre><code>F. Yu, W. Li, Q. Li, Y. Liu, X. Shi, J. Yan. POI: Multiple Object Tracking with
High Performance Detection and Appearance Feature. In BMTT, SenseTime Group
Limited, 2016.</code></pre>
<p>We have replaced the appearance descriptor with a custom deep convolutional neural network (see below).</p>
<h2 id="running-the-tracker">Running the tracker</h2>
<p>The following example starts the tracker on one of the <a href="https://motchallenge.net/data/MOT16/">MOT16 benchmark</a> sequences. We assume resources have been extracted to the repository root directory and the MOT16 benchmark data is in <code>./MOT16</code>:</p>
<pre><code>python deep_sort_app.py \
    --sequence_dir=./MOT16/test/MOT16-06 \
    --detection_file=./resources/detections/MOT16_POI_test/MOT16-06.npy \
    --min_confidence=0.3 \
    --nn_budget=100 \
    --display=True</code></pre>
<p>Check <code>python deep_sort_app.py -h</code> for an overview of available options. There are also scripts in the repository to visualize results, generate videos, and evaluate the MOT challenge benchmark.</p>
<h2 id="generating-detections">Generating detections</h2>
<p>Beside the main tracking application, this repository contains a script to generate features for person re-identification, suitable to compare the visual appearance of pedestrian bounding boxes using cosine similarity. The following example generates these features from standard MOT challenge detections. Again, we assume resources have been extracted to the repository root directory and MOT16 data is in <code>./MOT16</code>:</p>
<pre><code>python generate_detections.py
    --model=resources/networks/mars-small128.ckpt \
    --mot_dir=./MOT16/train \
    --output_dir=./resources/detections/MOT16_train</code></pre>
<p>For each sequence of the MOT16 dataset, the output is stored as separate binary file in NumPy native format. Each file contains an array of shape <code>Nx138</code>, where N is the number of detections in the corresponding MOT sequence. The first 10 columns of this array contain the raw MOT detection copied over from the input file. The remaining 128 columns store the appearance descriptor. The files generated by this command can be used as input for the <code>deep_sort_app.py</code>.</p>
<h2 id="highlevel-overview-of-source-files">Highlevel overview of source files</h2>
<p>In the top-level directory are executable scripts to execute, evaluate, and visualize the tracker. The main entry point is in <code>deep_sort_app.py</code>. This file runs the tracker on a MOTChallenge sequence.</p>
<p>In package <code>deep_sort</code> is the main tracking code:</p>
<ul>
<li><code>detection.py</code>: Detection base class.</li>
<li><code>kalman_filter.py</code>: A Kalman filter implementation and concrete parametrization for image space filtering.</li>
<li><code>linear_assignment.py</code>: This module contains code for min cost matching and the matching cascade.</li>
<li><code>iou_matching.py</code>: This module contains the IOU matching metric.</li>
<li><code>nn_matching.py</code>: A module for a nearest neighbor matching metric.</li>
<li><code>track.py</code>: The track class contains single-target track data such as Kalman state, number of hits, misses, hit streak, associated feature vectors, etc.</li>
<li><code>tracker.py</code>: This is the multi-target tracker class.</li>
</ul>
<p>The <code>deep_sort_app.py</code> expects detections in a custom format, stored in .npy files. These can be computed from MOTChallenge detections using <code>generate_detections.py</code>. We also provide <a href="https://owncloud.uni-koblenz.de/owncloud/s/f9JB0Jr7f3zzqs8">pre-generated detections</a>.</p>
<h2 id="citing-deepsort">Citing DeepSORT</h2>
<p>If you find this repo useful in your research, please consider citing the following papers:</p>
<pre><code>@article{Wojke2017simple,
  title={Simple Online and Realtime Tracking with a Deep Association Metric},
  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  journal={arXiv preprint arXiv:1703.07402},
  year={2017}
}

@inproceedings{Bewley2016_sort,
  author={Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
  title={Simple online and realtime tracking},
  year={2016},
  pages={3464-3468},
  doi={10.1109/ICIP.2016.7533003}
}</code></pre>
